#!/usr/bin/env python3
"""
Script to build the Search server database of documents.
Reads HTML files from inverted_index/crawl/, extracts metadata, and
creates a SQLite database at var/search.sqlite3 with a table:
  documents(docid INTEGER PRIMARY KEY,
            title VARCHAR(150),
            summary VARCHAR(250),
            url VARCHAR(150))
Usage:
  ./bin/searchdb
"""
import os
import sqlite3
import glob
from bs4 import BeautifulSoup

def get_summary(soup):
    """
    Extracts a summary for a document from the first <p> tag with
    sufficient text (over 50 chars). Truncates to 247 chars,
    escapes single quotes and newlines, and appends "...".
    """
    summary = ""
    p_elts = soup.find_all("p", class_=False)
    for p in p_elts:
        text = p.get_text()
        if text.strip() and len(text) > 50:
            # Truncate to 247 chars, then add ellipsis
            snippet = text.strip()[0:247]
            # Escape quotes and collapse newlines
            snippet = snippet.replace("\n", " ").replace("'", "''")
            summary = snippet + "..."
            break
    return summary

def main():
    # Default input directory for HTML crawl files
    input_dir = os.getenv("SEARCH_INPUT_DIR", os.path.join("inverted_index", "crawl"))

    # Output SQLite database path
    db_path = os.path.join("var", "search.sqlite3")

    # Ensure output directory exists
    os.makedirs(os.path.dirname(db_path), exist_ok=True)

    # Connect to SQLite and create table
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS documents (
            docid INTEGER PRIMARY KEY,
            title VARCHAR(150),
            summary VARCHAR(250),
            url VARCHAR(150)
        )
        """
    )

    # Iterate over all files in the input directory
    pattern = os.path.join(input_dir, "*")
    for filepath in sorted(glob.glob(pattern)):
        with open(filepath, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")

        # Extract metadata
        meta_doc = soup.find("meta", attrs={"eecs485_docid": True})
        meta_url = soup.find("meta", attrs={"eecs485_url": True})
        title_tag = soup.find("title")
        if not (meta_doc and meta_url and title_tag):
            continue

        # Parse fields
        docid_str = meta_doc.get("eecs485_docid")
        try:
            docid = int(docid_str)
        except (ValueError, TypeError):
            continue

        url = meta_url.get("eecs485_url")
        title_text = title_tag.get_text()
        # Remove suffix like " - Wikipedia"
        suffix = " - Wikipedia"
        if title_text.endswith(suffix):
            title_text = title_text[: -len(suffix)]
        title_text = title_text.strip()

        summary_text = get_summary(soup)

        # Insert or replace into the database
        c.execute(
            "INSERT OR REPLACE INTO documents (docid, title, summary, url) VALUES (?, ?, ?, ?)",
            (docid, title_text, summary_text, url)
        )

    # Commit and close
    conn.commit()
    conn.close()

    print(f"Created {db_path}")

if __name__ == "__main__":
    main()
